{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArsipKodeId/ArsipKodeId.github.io/blob/main/GenAI_GLI_02D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ps0LXjHsBLXy"
      },
      "source": [
        "# BRATS GLI GenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qWMzjEpqmWcZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "# Garbage collect Python objects\n",
        "gc.collect()\n",
        "\n",
        "# Clear PyTorch cache\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWlnM7h9AsZu"
      },
      "source": [
        "## Common Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ws3A7f7OL5x",
        "outputId": "215d8a0f-fc6e-4bd1-ded7-c90250540b5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: monai in /usr/local/lib/python3.12/dist-packages (1.5.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.24 in /usr/local/lib/python3.12/dist-packages (from monai) (2.0.2)\n",
            "Requirement already satisfied: torch>=2.4.1 in /usr/local/lib/python3.12/dist-packages (from monai) (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.4.1->monai) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.4.1->monai) (3.0.3)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.12/dist-packages (5.3.2)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.12/dist-packages (from nibabel) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from nibabel) (25.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.12/dist-packages (from nibabel) (4.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install monai\n",
        "!pip install nibabel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_F63OgxNkB2",
        "outputId": "2b054c03-1d06-438c-ead7-c58af38de015"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 'get_modality_paths' function has been updated to exclude 'seg'.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "def get_modality_paths(patient_folder_path):\n",
        "  \"\"\"\n",
        "  Generates a dictionary of full file paths for MRI modalities and segmentation masks\n",
        "  for a given patient folder.\n",
        "\n",
        "  Args:\n",
        "    patient_folder_path (str): The path to the patient's data folder.\n",
        "\n",
        "  Returns:\n",
        "    dict: A dictionary where keys are modality names (t1c, t1n, t2f, t2w, seg)\n",
        "          and values are their corresponding full file paths.\n",
        "  \"\"\"\n",
        "  modalities = ['t1c', 't1n', 't2f', 't2w'] # Excluded 'seg' as per user request\n",
        "  modality_paths = {}\n",
        "\n",
        "  # Extract patient_id from the patient_folder_path\n",
        "  patient_id = os.path.basename(patient_folder_path)\n",
        "\n",
        "  for modality in modalities:\n",
        "    file_name = f\"{patient_id}-{modality}.nii.gz\"\n",
        "    full_path = os.path.join(patient_folder_path, file_name)\n",
        "    modality_paths[modality] = full_path\n",
        "\n",
        "  return modality_paths\n",
        "\n",
        "print(\"The 'get_modality_paths' function has been updated to exclude 'seg'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXgoSgXQma6V",
        "outputId": "8d7de234-c2c0-4b08-dc0c-b308daf3cfce"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NXlVwUmGOAtG"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "tumor_img_start = 15\n",
        "tumor_img_end = 111\n",
        "\n",
        "root_path_50 = \"/content/drive/MyDrive/Datasets/BraTS2023_GLI_Challenge/training_50\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EJutjp6qOHHY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f24e5a3-2ef4-49bd-d2b8-4d5294f4b4b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<frozen importlib._bootstrap_external>:1301: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n"
          ]
        }
      ],
      "source": [
        "from monai.transforms import HistogramNormalize\n",
        "import nibabel as nib # Added for self-containment\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3o21nqcIOSbX",
        "outputId": "21e67b74-07fc-476a-a976-c611ead4d6dc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "folders = glob.glob(f\"{root_path_50}/***\")\n",
        "len(folders)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QWpAYmPmWcj",
        "outputId": "08222639-090c-4cf1-cdc1-3dc4ba486da4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "folders = folders[:17]\n",
        "len(folders)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7x4A51FAzwA"
      },
      "source": [
        "## Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c45a147",
        "outputId": "debddf2e-74f7-484b-cafc-33b17e96e4c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The BraTSDataset class has been defined.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class BraTSDataset(Dataset):\n",
        "  def __init__(self, patient_folders):\n",
        "    self.patient_folders = patient_folders\n",
        "    print(f\"Initialized BraTSDataset with {len(self.patient_folders)} patient folders.\")\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.patient_folders)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # Placeholder for loading and preprocessing data for a single patient\n",
        "    # This will be implemented in subsequent steps.\n",
        "    patient_folder_path = self.patient_folders[index]\n",
        "    # For now, just return the path to demonstrate it's working\n",
        "    return patient_folder_path\n",
        "\n",
        "print(\"The BraTSDataset class has been defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75b84f6f",
        "outputId": "b1c80cbe-86c3-4848-d544-f22b03626f3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The BraTSDataset class has been updated to yield individual 2D axial slices for image-to-image translation.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from monai.transforms import HistogramNormalize, ScaleIntensityRange\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "\n",
        "class BraTSDataset(Dataset):\n",
        "  def __init__(self, patient_folders):\n",
        "    self.patient_folders = patient_folders\n",
        "    self.histogram_normalize = HistogramNormalize(num_bins=256)\n",
        "    self.num_slices_per_volume = tumor_img_end - tumor_img_start\n",
        "    print(f\"Initialized BraTSDataset with {len(self.patient_folders)} patient folders.\")\n",
        "\n",
        "  def __len__(self):\n",
        "    # Now returns the total number of 2D slices across all patients\n",
        "    return len(self.patient_folders) * self.num_slices_per_volume\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # Determine which patient and which slice within that patient corresponds to the index\n",
        "    patient_idx = index // self.num_slices_per_volume\n",
        "    slice_in_volume_idx = index % self.num_slices_per_volume\n",
        "    actual_slice_idx = tumor_img_start + slice_in_volume_idx\n",
        "\n",
        "    patient_folder_path = self.patient_folders[patient_idx]\n",
        "    modality_paths = get_modality_paths(patient_folder_path)\n",
        "\n",
        "    # Load NIfTI images for each modality\n",
        "    t1c_img = nib.load(modality_paths['t1c']).get_fdata()\n",
        "    t1n_img = nib.load(modality_paths['t1n']).get_fdata()\n",
        "    t2f_img = nib.load(modality_paths['t2f']).get_fdata()\n",
        "    t2w_img = nib.load(modality_paths['t2w']).get_fdata()\n",
        "\n",
        "    # Convert numpy arrays to torch tensors\n",
        "    t1c_tensor = torch.from_numpy(t1c_img).float()\n",
        "    t1n_tensor = torch.from_numpy(t1n_img).float()\n",
        "    t2f_tensor = torch.from_numpy(t2f_img).float()\n",
        "    t2w_tensor = torch.from_numpy(t2w_img).float()\n",
        "\n",
        "    # Apply HistogramNormalize (Monai's HistogramNormalize expects channel-first, so unsqueeze for 3D image)\n",
        "    t1c_normalized = self.histogram_normalize(t1c_tensor.unsqueeze(0)).squeeze(0)\n",
        "    t1n_normalized = self.histogram_normalize(t1n_tensor.unsqueeze(0)).squeeze(0)\n",
        "    t2f_normalized = self.histogram_normalize(t2f_tensor.unsqueeze(0)).squeeze(0)\n",
        "    t2w_normalized = self.histogram_normalize(t2w_tensor.unsqueeze(0)).squeeze(0)\n",
        "\n",
        "    # Apply ScaleIntensityRange for min-max scaling to [0, 1] for each modality\n",
        "    def monai_min_max_scale(tensor):\n",
        "      min_val = tensor.min()\n",
        "      max_val = tensor.max()\n",
        "      if max_val - min_val == 0:\n",
        "        return torch.zeros_like(tensor)\n",
        "      # Create ScaleIntensityRange for this specific tensor's min/max\n",
        "      scale_transform = ScaleIntensityRange(a_min=min_val, a_max=max_val, b_min=0.0, b_max=1.0, clip=True)\n",
        "      return scale_transform(tensor)\n",
        "\n",
        "    t1c_scaled = monai_min_max_scale(t1c_normalized)\n",
        "    t1n_scaled = monai_min_max_scale(t1n_normalized)\n",
        "    t2f_scaled = monai_min_max_scale(t2f_normalized)\n",
        "    t2w_scaled = monai_min_max_scale(t2w_normalized)\n",
        "\n",
        "    # Extract a single axial slice at actual_slice_idx\n",
        "    t1c_slice = t1c_scaled[:, :, actual_slice_idx]\n",
        "    t1n_slice = t1n_scaled[:, :, actual_slice_idx]\n",
        "    t2f_slice = t2f_scaled[:, :, actual_slice_idx]\n",
        "    t2w_slice = t2w_scaled[:, :, actual_slice_idx]\n",
        "\n",
        "    # Stack preprocessed MRI modalities into an input tensor (t1n, t2f, t2w)\n",
        "    # Input tensor will have shape (C, H, W) where C=3 for 3 modalities\n",
        "    input_tensor = torch.stack([t1n_slice, t2f_slice, t2w_slice], dim=0)\n",
        "\n",
        "    # Target tensor for t1c (image to image translation)\n",
        "    # Add channel dimension to the 2D slice\n",
        "    target_tensor = t1c_slice.unsqueeze(0)\n",
        "\n",
        "    return input_tensor, target_tensor\n",
        "\n",
        "print(\"The BraTSDataset class has been updated to yield individual 2D axial slices for image-to-image translation.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kc5_F8gFSXib",
        "outputId": "0639549b-d0c2-4226-8cce-bb4c861a010d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized BraTSDataset with 17 patient folders.\n",
            "Initialized DataLoader with 1632 samples and batch size 8.\n",
            "Shape of input tensor: torch.Size([8, 3, 240, 240])\n",
            "Shape of target tensor: torch.Size([8, 1, 240, 240])\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Instantiate the BraTSDataset\n",
        "brats_dataset = BraTSDataset(folders)\n",
        "\n",
        "# Create a DataLoader\n",
        "batch_size = 8 # For demonstration, we'll use a batch size of 1\n",
        "data_loader = DataLoader(brats_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Initialized DataLoader with {len(brats_dataset)} samples and batch size {batch_size}.\")\n",
        "\n",
        "# Get a single sample from the data loader to verify\n",
        "for i, (input_tensor, target_tensor) in enumerate(data_loader):\n",
        "  if i == 0:\n",
        "    print(f\"Shape of input tensor: {input_tensor.shape}\")\n",
        "    print(f\"Shape of target tensor: {target_tensor.shape}\")\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tcxuw2XwSsRj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILpKjOAxA4yx"
      },
      "source": [
        "## Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### UNet Generator + PatchGAN Discriminator"
      ],
      "metadata": {
        "id": "O_WVgqP6zrAe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUm1_rD0Rm05",
        "outputId": "f798b898-0574-4889-a999-f267886389ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UNet model for Generator defined successfully.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "  \"\"\"Helper block for two convolutional layers with Batch Normalization and ReLU.\"\"\"\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super().__init__()\n",
        "    self.double_conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.double_conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "  \"\"\"Downsampling block with MaxPool and DoubleConv.\"\"\"\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super().__init__()\n",
        "    self.maxpool_conv = nn.Sequential(\n",
        "        nn.MaxPool2d(2),\n",
        "        DoubleConv(in_channels, out_channels)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.maxpool_conv(x)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "  \"\"\"Upsampling block with TransposedConv, concatenation, and DoubleConv.\"\"\"\n",
        "  def __init__(self, in_channels, out_channels, bilinear=False):\n",
        "    super().__init__()\n",
        "\n",
        "    # if bilinear, use the normal convolutions to reduce the number of channels\n",
        "    if bilinear:\n",
        "      self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "      self.conv = DoubleConv(in_channels, out_channels)\n",
        "    else:\n",
        "      self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
        "      self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "  def forward(self, x1, x2):\n",
        "    x1 = self.up(x1)\n",
        "    # input is CHW\n",
        "    diffY = x2.size()[2] - x1.size()[2]\n",
        "    diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "    x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                    diffY // 2, diffY - diffY // 2])\n",
        "    # If you have padding issues, see\n",
        "    # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
        "    # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
        "    x = torch.cat([x2, x1], dim=1)\n",
        "    return self.conv(x)\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "  \"\"\"Full U-Net architecture for image segmentation or image-to-image translation.\"\"\"\n",
        "  def __init__(self, n_channels, n_classes, bilinear=False):\n",
        "    super(UNet, self).__init__()\n",
        "    self.n_channels = n_channels\n",
        "    self.n_classes = n_classes\n",
        "    self.bilinear = bilinear\n",
        "\n",
        "    # Encoder (Downsampling path)\n",
        "    self.inc = DoubleConv(n_channels, 64)\n",
        "    self.down1 = Down(64, 128)\n",
        "    self.down2 = Down(128, 256)\n",
        "    self.down3 = Down(256, 512)\n",
        "    factor = 2 if bilinear else 1\n",
        "    self.down4 = Down(512, 1024 // factor)\n",
        "\n",
        "    # Decoder (Upsampling path)\n",
        "    self.up1 = Up(1024, 512 // factor, bilinear)\n",
        "    self.up2 = Up(512, 256 // factor, bilinear)\n",
        "    self.up3 = Up(256, 128 // factor, bilinear)\n",
        "    self.up4 = Up(128, 64, bilinear)\n",
        "    self.outc = nn.Conv2d(64, n_classes, kernel_size=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x1 = self.inc(x)\n",
        "    x2 = self.down1(x1)\n",
        "    x3 = self.down2(x2)\n",
        "    x4 = self.down3(x3)\n",
        "    x5 = self.down4(x4)\n",
        "\n",
        "    x = self.up1(x5, x4) # Concatenate with skip connection x4\n",
        "    x = self.up2(x, x3)  # Concatenate with skip connection x3\n",
        "    x = self.up3(x, x2)  # Concatenate with skip connection x2\n",
        "    x = self.up4(x, x1)  # Concatenate with skip connection x1\n",
        "    logits = self.outc(x)\n",
        "    return logits\n",
        "\n",
        "print(\"UNet model for Generator defined successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16119a72",
        "outputId": "74056cad-457d-426a-f065-47cde8ab98ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UNet Generator instantiated.\n",
            "Input tensor shape: torch.Size([8, 3, 240, 240])\n",
            "Output tensor shape from Generator: torch.Size([8, 1, 240, 240])\n",
            "Target tensor shape: torch.Size([8, 1, 240, 240])\n",
            "Generator output shape verified.\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the UNet Generator\n",
        "# Input channels: 3 (t1n, t2f, t2w)\n",
        "# Output channels: 1 (t1c)\n",
        "\n",
        "generator = UNet(n_channels=3, n_classes=1)\n",
        "\n",
        "print(\"UNet Generator instantiated.\")\n",
        "\n",
        "# Get a single batch from the DataLoader\n",
        "for i, (input_tensor, target_tensor) in enumerate(data_loader):\n",
        "  if i == 0:\n",
        "    # Move tensors to the same device as the model if using GPU\n",
        "    # For now, assuming CPU\n",
        "    # input_tensor = input_tensor.to(device)\n",
        "    # target_tensor = target_tensor.to(device)\n",
        "\n",
        "    # Pass the input through the generator\n",
        "    output_tensor = generator(input_tensor)\n",
        "\n",
        "    print(f\"Input tensor shape: {input_tensor.shape}\")\n",
        "    print(f\"Output tensor shape from Generator: {output_tensor.shape}\")\n",
        "    print(f\"Target tensor shape: {target_tensor.shape}\")\n",
        "    break\n",
        "\n",
        "print(\"Generator output shape verified.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfbbsoxyWE4b",
        "outputId": "1fb76f26-4b1e-4197-9f39-35f18732fd56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PatchGAN Discriminator architecture defined successfully.\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "  \"\"\"PatchGAN Discriminator architecture.\"\"\"\n",
        "  def __init__(self, in_channels=4, features=[64, 128, 256, 512]):\n",
        "    super().__init__()\n",
        "    # The discriminator takes both input image (3 channels) and target image (1 channel)\n",
        "    # concatenated as input, so in_channels will be 3+1 = 4.\n",
        "    self.initial_block = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, features[0], kernel_size=4, stride=2, padding=1, bias=False),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "    )\n",
        "\n",
        "    layers = []\n",
        "    # Downsampling layers\n",
        "    for i in range(len(features) - 1):\n",
        "        in_f = features[i]\n",
        "        out_f = features[i+1]\n",
        "        layers += [\n",
        "            nn.Conv2d(in_f, out_f, kernel_size=4, stride=2 if i != len(features) - 2 else 1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_f),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        ]\n",
        "\n",
        "    # Final output layer to produce a 1-channel output (probability map)\n",
        "    layers += [\n",
        "        nn.Conv2d(features[-1], 1, kernel_size=4, stride=1, padding=1, bias=False)\n",
        "    ]\n",
        "\n",
        "    self.model = nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.initial_block(x)\n",
        "    return self.model(x)\n",
        "\n",
        "print(\"PatchGAN Discriminator architecture defined successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSqvRWKVV_xA",
        "outputId": "cfeee58f-0296-4cce-dc7f-5006e6c0c235"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PatchGAN Discriminator instantiated.\n",
            "Discriminator input tensor shape: torch.Size([8, 4, 240, 240])\n",
            "Discriminator output tensor shape: torch.Size([8, 1, 28, 28])\n",
            "Discriminator output shape verified.\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the PatchGAN Discriminator\n",
        "# Input channels: 3 (from input modalities) + 1 (from target/generated t1c) = 4\n",
        "discriminator = Discriminator(in_channels=4)\n",
        "\n",
        "print(\"PatchGAN Discriminator instantiated.\")\n",
        "\n",
        "# Get a single batch from the DataLoader to test the discriminator\n",
        "for i, (input_tensor, target_tensor) in enumerate(data_loader):\n",
        "  if i == 0:\n",
        "    # Concatenate the input_tensor and target_tensor along the channel dimension\n",
        "    # input_tensor has shape (batch_size, 3, H, W)\n",
        "    # target_tensor has shape (batch_size, 1, H, W)\n",
        "    # Discriminator input should have shape (batch_size, 4, H, W)\n",
        "    discriminator_input = torch.cat([input_tensor, target_tensor], dim=1)\n",
        "\n",
        "    # Pass the concatenated input through the discriminator\n",
        "    discriminator_output = discriminator(discriminator_input)\n",
        "\n",
        "    print(f\"Discriminator input tensor shape: {discriminator_input.shape}\")\n",
        "    print(f\"Discriminator output tensor shape: {discriminator_output.shape}\")\n",
        "    break\n",
        "\n",
        "print(\"Discriminator output shape verified.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GAN as Generator and Discriminator"
      ],
      "metadata": {
        "id": "-FU5rVToz9LA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uITqqibp0BcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uAXMrMOp0A53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ5AEbPQA8uP"
      },
      "source": [
        "## Loss Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d3536e0",
        "outputId": "bc6d7667-2121-43b9-ae9a-f2ed19fd9ad7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L1 Loss, Adversarial Loss (BCEWithLogitsLoss), and MSE Loss instantiated successfully.\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# 2. Instantiate L1Loss\n",
        "l1_loss = nn.L1Loss()\n",
        "\n",
        "# 3. Instantiate BCEWithLogitsLoss for adversarial loss\n",
        "adversarial_loss = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# 4. Instantiate MSELoss\n",
        "mse_loss = nn.MSELoss()\n",
        "\n",
        "print(\"L1 Loss, Adversarial Loss (BCEWithLogitsLoss), and MSE Loss instantiated successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xi5ha1w4A_Hh"
      },
      "source": [
        "## Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9kb-EUsXETw",
        "outputId": "71b24f8d-b998-4b42-caaa-127e844cec56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PSNR and SSIM calculation functions defined successfully using MONAI Metric classes.\n"
          ]
        }
      ],
      "source": [
        "from monai.metrics import PSNRMetric, SSIMMetric\n",
        "import torch\n",
        "\n",
        "# Instantiate the metric classes once\n",
        "# PSNRMetric requires 'max_val' to be specified during initialization.\n",
        "psnr_metric_calculator = PSNRMetric(max_val=1.0)\n",
        "# Removed 'size_average', 'gaussian_weights', 'K1', 'K2', 'kernel_size', 'sigma' as they're causing TypeErrors\n",
        "ssim_metric_calculator = SSIMMetric(spatial_dims=2)\n",
        "\n",
        "def calculate_psnr(predicted_image, target_image):\n",
        "  \"\"\"\n",
        "  Calculates the Peak Signal-to-Noise Ratio (PSNR) between two PyTorch tensors.\n",
        "  Args:\n",
        "    predicted_image (torch.Tensor): The predicted image tensor (B, C, H, W).\n",
        "    target_image (torch.Tensor): The target image tensor (B, C, H, W).\n",
        "  Returns:\n",
        "    torch.Tensor: The PSNR value.\n",
        "  \"\"\"\n",
        "  # PSNRMetric expects lists of tensors\n",
        "  return psnr_metric_calculator([predicted_image], [target_image])\n",
        "\n",
        "def calculate_ssim(predicted_image, target_image):\n",
        "  \"\"\"\n",
        "  Calculates the Structural Similarity Index Measure (SSIM) between two PyTorch tensors.\n",
        "  Args:\n",
        "    predicted_image (torch.Tensor): The predicted image tensor (B, C, H, W).\n",
        "    target_image (torch.Tensor): The target image tensor (B, C, H, W).\n",
        "  Returns:\n",
        "    torch.Tensor: The SSIM value.\n",
        "  \"\"\"\n",
        "  # SSIMMetric expects lists of tensors, and 'data_range' is no longer passed as it caused a TypeError\n",
        "  # Assuming images are already normalized to [0, 1]\n",
        "  return ssim_metric_calculator([predicted_image], [target_image])\n",
        "\n",
        "print(\"PSNR and SSIM calculation functions defined successfully using MONAI Metric classes.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gm8FkqFFBDiD"
      },
      "source": [
        "## Params Fine Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "428b063d",
        "outputId": "343638ee-56b7-413f-9617-899cf2e407ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adam optimizers for Generator and Discriminator initialized successfully.\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Learning rates (these are typical starting points for GANs)\n",
        "lr_g = 2e-4  # Learning rate for the Generator\n",
        "lr_d = 2e-4  # Learning rate for the Discriminator\n",
        "\n",
        "# Adam optimizers for Generator and Discriminator\n",
        "optimizer_g = optim.Adam(generator.parameters(), lr=lr_g, betas=(0.5, 0.999))\n",
        "optimizer_d = optim.Adam(discriminator.parameters(), lr=lr_d, betas=(0.5, 0.999))\n",
        "\n",
        "print(\"Adam optimizers for Generator and Discriminator initialized successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d567929",
        "outputId": "a9e8c366-481b-4481-81df-ea60e08c3b95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training will be performed on: cuda\n",
            "Number of epochs: 1\n",
            "Lambda for L1 loss: 100\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Move models to the selected device\n",
        "generator.to(device)\n",
        "discriminator.to(device)\n",
        "\n",
        "# Training parameters\n",
        "num_epochs = 1\n",
        "lambda_l1 = 100 # Weight for L1 loss in the Generator's objective (as in Pix2Pix paper)\n",
        "\n",
        "print(f\"Training will be performed on: {device}\")\n",
        "print(f\"Number of epochs: {num_epochs}\")\n",
        "print(f\"Lambda for L1 loss: {lambda_l1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mar_vWsXBG9u"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6D2u2RcPNJpR",
        "outputId": "0c8b185c-2f74-4acd-b822-8d08a6fc73f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pix2Pix"
      ],
      "metadata": {
        "id": "6RWJhGr6zneK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "id": "XBUPYtdRcP-l",
        "outputId": "d067e085-d6d6-4d26-a7ab-ddbf86540c26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Only one GPU available. To prevent OutOfMemoryError, it's strongly recommended to reduce the 'batch_size' in cell 'Kc5_F8gFSXib' (e.g., to 8 or 4).\n",
            "Attempting to clear CUDA cache and collect garbage as a temporary mitigation.\n",
            "Starting training loop setup.\n",
            "Epoch 1/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1:  61%|██████▏   | 125/204 [42:36<26:55, 20.45s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1355048899.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m   \u001b[0;31m# Iterate through the data_loader with tqdm for progress tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;31m# Move tensors to the device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2745564443.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mt1c_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodality_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't1c'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_fdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mt1n_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodality_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't1n'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_fdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mt2f_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodality_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't2f'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_fdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mt2w_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodality_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't2w'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_fdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nibabel/dataobj_images.py\u001b[0m in \u001b[0;36mget_fdata\u001b[0;34m(self, caching, dtype)\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;31m# For array proxies, will attempt to confine data array to dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0;31m# during scaling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcaching\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'fill'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fdata_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nibabel/arrayproxy.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0mScaled\u001b[0m \u001b[0mimage\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m         \"\"\"\n\u001b[0;32m--> 454\u001b[0;31m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_scaled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslicer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nibabel/arrayproxy.py\u001b[0m in \u001b[0;36m_get_scaled\u001b[0;34m(self, dtype, slicer)\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0mscl_inter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscl_inter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;31m# Read array and upcast as necessary for big slopes, intercepts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         \u001b[0mscaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_read_scaling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_unscaled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslicer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslicer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscl_slope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscl_inter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0mscaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpromote_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nibabel/arrayproxy.py\u001b[0m in \u001b[0;36m_get_unscaled\u001b[0;34m(self, slicer)\u001b[0m\n\u001b[1;32m    389\u001b[0m         ):\n\u001b[1;32m    390\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_fileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                 return array_from_file(\n\u001b[0m\u001b[1;32m    392\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nibabel/volumeutils.py\u001b[0m in \u001b[0;36marray_from_file\u001b[0;34m(shape, in_dtype, infile, offset, order, mmap)\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'readinto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0mdata_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m         \u001b[0mn_read\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m         \u001b[0mneeds_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEBADF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read() on write-only GzipFile object\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/_compression.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbyte_view\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_view\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mbyte_view\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0;31m# Read a chunk of data from the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneeds_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mREAD_BUFFER_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m                 \u001b[0muncompress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mread\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd # Import pandas for CSV handling\n",
        "import gc # Import garbage collector for OOM mitigation\n",
        "\n",
        "# --- Multi-GPU Setup (if available) and OOM mitigation ---\n",
        "# If multiple GPUs are available, wrap the models with nn.DataParallel.\n",
        "# This part is ideally placed once after model instantiation and moving to device,\n",
        "# typically in a cell like '9d567929'. It's placed here to adhere to the\n",
        "# constraint of modifying only this cell. Repeated execution of this cell\n",
        "# will inefficiently re-wrap the models.\n",
        "if torch.cuda.device_count() > 1 and not isinstance(generator, torch.nn.DataParallel):\n",
        "    print(f\"Wrapping Generator and Discriminator with nn.DataParallel for {torch.cuda.device_count()} GPUs.\")\n",
        "    generator = torch.nn.DataParallel(generator)\n",
        "    discriminator = torch.nn.DataParallel(discriminator)\n",
        "    # The inputs should still be sent to the primary device (cuda:0).\n",
        "    # nn.DataParallel handles scattering the data to other GPUs.\n",
        "elif torch.cuda.device_count() == 1:\n",
        "    print(\"Only one GPU available. To prevent OutOfMemoryError, it's strongly recommended to reduce the 'batch_size' in cell 'Kc5_F8gFSXib' (e.g., to 8 or 4).\")\n",
        "    print(\"Attempting to clear CUDA cache and collect garbage as a temporary mitigation.\")\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "else:\n",
        "    print(\"No GPU available. Training will proceed on CPU, which may be significantly slower.\")\n",
        "\n",
        "print(\"Starting training loop setup.\")\n",
        "\n",
        "# Adjust num_epochs for a more complete training run\n",
        "num_epochs = 25\n",
        "\n",
        "# Lists to store losses and metrics for plotting/logging\n",
        "g_losses = []\n",
        "d_losses = []\n",
        "psnr_scores = []\n",
        "ssim_scores = []\n",
        "\n",
        "# List to store all epoch results for CSV export\n",
        "epoch_results = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "  # Set models to training mode\n",
        "  generator.train()\n",
        "  discriminator.train()\n",
        "\n",
        "  # Initialize epoch-wise loss accumulation\n",
        "  epoch_g_loss = 0.0\n",
        "  epoch_d_loss = 0.0\n",
        "  epoch_gen_adversarial_loss = 0.0\n",
        "  epoch_gen_l1_loss = 0.0\n",
        "  num_batches = 0\n",
        "\n",
        "  # Iterate through the data_loader with tqdm for progress tracking\n",
        "  for batch_idx, (input_tensor, target_tensor) in enumerate(tqdm(data_loader, desc=f\"Epoch {epoch+1}\")):\n",
        "    # Move tensors to the device\n",
        "    input_tensor = input_tensor.to(device)\n",
        "    target_tensor = target_tensor.to(device);\n",
        "\n",
        "    # --- Discriminator training step ---\n",
        "    optimizer_d.zero_grad()\n",
        "\n",
        "    # 1. Train with real images\n",
        "    # Concatenate input_tensor and original target_tensor for real input\n",
        "    real_discriminator_input = torch.cat([input_tensor, target_tensor], dim=1);\n",
        "    real_output = discriminator(real_discriminator_input);\n",
        "    # Create a tensor of ones for real labels\n",
        "    real_labels = torch.ones_like(real_output).to(device);\n",
        "    d_real_loss = adversarial_loss(real_output, real_labels);\n",
        "\n",
        "    # 2. Train with fake images\n",
        "    # Generate fake target_tensor\n",
        "    fake_target_tensor = generator(input_tensor);\n",
        "    # Detach fake_target_tensor to prevent gradients from flowing back to the generator\n",
        "    fake_discriminator_input = torch.cat([input_tensor, fake_target_tensor.detach()], dim=1);\n",
        "    fake_output = discriminator(fake_discriminator_input);\n",
        "    # Create a tensor of zeros for fake labels\n",
        "    fake_labels = torch.zeros_like(fake_output).to(device);\n",
        "    d_fake_loss = adversarial_loss(fake_output, fake_labels);\n",
        "\n",
        "    # 3. Combine losses and update Discriminator\n",
        "    d_loss = d_real_loss + d_fake_loss;\n",
        "    d_loss.backward();\n",
        "    optimizer_d.step();\n",
        "    # --- End Discriminator training step ---\n",
        "\n",
        "    # Add memory clean-up after discriminator step\n",
        "    del real_discriminator_input, real_output, real_labels, fake_discriminator_input, fake_output, fake_labels\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    # --- Generator training step ---\n",
        "    optimizer_g.zero_grad()\n",
        "\n",
        "    # Generate fake target_tensor (already done above, but we need gradients now)\n",
        "    fake_target_tensor_for_gen = generator(input_tensor);\n",
        "\n",
        "    # 1. Adversarial loss for Generator (Generator wants discriminator to think fakes are real)\n",
        "    # Discriminator input with fake images for generator loss calculation\n",
        "    gen_discriminator_input = torch.cat([input_tensor, fake_target_tensor_for_gen], dim=1);\n",
        "    # Re-create real_labels for generator's adversarial loss as it wants fakes to be seen as real\n",
        "    real_labels_for_gen = torch.ones_like(discriminator(gen_discriminator_input)).to(device)\n",
        "    gen_output = discriminator(gen_discriminator_input);\n",
        "    # Generator wants discriminator to output ones for fake images\n",
        "    gen_adversarial_loss = adversarial_loss(gen_output, real_labels_for_gen);\n",
        "\n",
        "    # 2. L1 loss (reconstruction loss)\n",
        "    gen_l1_loss = l1_loss(fake_target_tensor_for_gen, target_tensor);\n",
        "\n",
        "    # 3. Combine losses and update Generator\n",
        "    g_loss = gen_adversarial_loss + lambda_l1 * gen_l1_loss;\n",
        "    g_loss.backward();\n",
        "    optimizer_g.step();\n",
        "    # --- End Generator training step ---\n",
        "\n",
        "    # Add memory clean-up after generator step\n",
        "    del fake_target_tensor, fake_target_tensor_for_gen, gen_discriminator_input, gen_output, real_labels_for_gen\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    epoch_g_loss += g_loss.item();\n",
        "    epoch_d_loss += d_loss.item();\n",
        "    epoch_gen_adversarial_loss += gen_adversarial_loss.item();\n",
        "    epoch_gen_l1_loss += gen_l1_loss.item();\n",
        "    num_batches += 1;\n",
        "\n",
        "  # --- Evaluation and Logging after each epoch ---\n",
        "  generator.eval()\n",
        "  discriminator.eval() # Discriminator eval mode, though not strictly necessary for metrics\n",
        "\n",
        "  total_psnr = 0.0\n",
        "  total_ssim = 0.0\n",
        "  total_mse_eval_loss = 0.0\n",
        "  eval_samples = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    # Add memory clean-up before evaluation loop\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    for input_tensor_eval, target_tensor_eval in data_loader:\n",
        "      input_tensor_eval = input_tensor_eval.to(device);\n",
        "      target_tensor_eval = target_tensor_eval.to(device);\n",
        "\n",
        "      generated_image = generator(input_tensor_eval);\n",
        "\n",
        "      # Fix: Squeeze the batch dimension (N=1) to convert (1, C, H, W) to (C, H, W)\n",
        "      # This is because MONAI's metric classes internally add a batch dimension for each item in the list.\n",
        "      # If batch_size > 1, take the first image in the batch for metric calculation or loop through the batch.\n",
        "      # For simplicity, we'll calculate metrics for the entire batch and average.\n",
        "      # MONAI's metrics expect a list of NCHW tensors, so we pass [tensor] where tensor is NCHW.\n",
        "      current_psnr = calculate_psnr(generated_image, target_tensor_eval);\n",
        "      current_ssim = calculate_ssim(generated_image, target_tensor_eval);\n",
        "      current_mse_loss = mse_loss(generated_image, target_tensor_eval);\n",
        "\n",
        "      total_psnr += current_psnr.item() * input_tensor_eval.size(0);\n",
        "      total_ssim += current_ssim.item() * input_tensor_eval.size(0);\n",
        "      total_mse_eval_loss += current_mse_loss.item() * input_tensor_eval.size(0);\n",
        "      eval_samples += input_tensor_eval.size(0);\n",
        "\n",
        "      # Add memory clean-up after each evaluation batch\n",
        "      del input_tensor_eval, target_tensor_eval, generated_image\n",
        "      torch.cuda.empty_cache()\n",
        "      gc.collect()\n",
        "\n",
        "  avg_psnr = total_psnr / eval_samples;\n",
        "  avg_ssim = total_ssim / eval_samples;\n",
        "  avg_mse_eval_loss = total_mse_eval_loss / eval_samples;\n",
        "\n",
        "  # Store losses and metrics\n",
        "  avg_g_loss = epoch_g_loss / num_batches;\n",
        "  avg_d_loss = epoch_d_loss / num_batches;\n",
        "  avg_gen_adversarial_loss = epoch_gen_adversarial_loss / num_batches;\n",
        "  avg_gen_l1_loss = epoch_gen_l1_loss / num_batches;\n",
        "\n",
        "  g_losses.append(avg_g_loss);\n",
        "  d_losses.append(avg_d_loss);\n",
        "  psnr_scores.append(avg_psnr);\n",
        "  ssim_scores.append(avg_ssim);\n",
        "\n",
        "  # Store epoch results for CSV\n",
        "  epoch_results.append({\n",
        "      'Epoch': epoch + 1,\n",
        "      'Avg_Generator_Loss': avg_g_loss,\n",
        "      'Avg_Discriminator_Loss': avg_d_loss,\n",
        "      'Avg_Gen_Adversarial_Loss': avg_gen_adversarial_loss,\n",
        "      'Avg_Gen_L1_Loss': avg_gen_l1_loss,\n",
        "      'Avg_MSE_Loss_Eval': avg_mse_eval_loss,\n",
        "      'Avg_PSNR': avg_psnr,\n",
        "      'Avg_SSIM': avg_ssim\n",
        "  });\n",
        "\n",
        "  print(f\"Epoch {epoch+1} Results:\")\n",
        "  print(f\"  Avg Generator Loss: {avg_g_loss:.4f}\")\n",
        "  print(f\"  Avg Discriminator Loss: {avg_d_loss:.4f}\")\n",
        "  print(f\"  Avg Gen Adversarial Loss: {avg_gen_adversarial_loss:.4f}\")\n",
        "  print(f\"  Avg Gen L1 Loss: {avg_gen_l1_loss:.4f}\")\n",
        "  print(f\"  Avg MSE Loss (Eval): {avg_mse_eval_loss:.4f}\")\n",
        "  print(f\"  Avg PSNR: {avg_psnr:.4f}\")\n",
        "  print(f\"  Avg SSIM: {avg_ssim:.4f}\")\n",
        "  # --- End Evaluation and Logging ---\n",
        "\n",
        "print(\"\\nTraining complete.\")\n",
        "\n",
        "# Save training metrics to CSV\n",
        "df_results = pd.DataFrame(epoch_results);\n",
        "df_results.to_csv('training_metrics.csv', index=False);\n",
        "print(\"Training metrics saved to training_metrics.csv\")\n",
        "\n",
        "# Save the models\n",
        "if isinstance(generator, torch.nn.DataParallel):\n",
        "    torch.save(generator.module.state_dict(), 'generator_model.pth')\n",
        "    torch.save(discriminator.module.state_dict(), 'discriminator_model.pth')\n",
        "else:\n",
        "    torch.save(generator.state_dict(), 'generator_model.pth')\n",
        "    torch.save(discriminator.state_dict(), 'discriminator_model.pth')\n",
        "print(\"Generator and Discriminator models saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WKGw9R9iM_Z"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}